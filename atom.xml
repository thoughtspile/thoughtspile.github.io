<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Vladimir Klepov as a Coder</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://thoughtspile.github.io/"/>
  <updated>2018-07-07T07:56:33.000Z</updated>
  <id>https://thoughtspile.github.io/</id>
  
  <author>
    <name>Vladimir Klepov</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Advanced Promise Coordination: Rate Limiting</title>
    <link href="https://thoughtspile.github.io/2018/07/07/rate-limit-promises/"/>
    <id>https://thoughtspile.github.io/2018/07/07/rate-limit-promises/</id>
    <published>2018-07-07T07:11:13.000Z</published>
    <updated>2018-07-07T07:56:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>In the <a href="/2018/06/20/serialize-promises/">previous post</a> we learnt to serialize<br>and concurrecy-limit promise-based operations in js. This time we dive further<br>and handle rate limiting.</p><h2 id="What-Exactly-to-Rate-Limit"><a href="#What-Exactly-to-Rate-Limit" class="headerlink" title="What Exactly to Rate Limit"></a>What Exactly to Rate Limit</h2><p>Let’s get terminological matters out of the way first. Promises represent operations<br>that last a certain amount of time, while rate limiting is applied to discrete events.<br>Over its life, a promise starts and terminates (with a success or a failure, not<br>important now). It makes most sense to rate limit promise creations (starts).<br>Rate limiting promise resolutions can be done by appending a start-rate-limited<br>promise onto the end of the running promise. We could also limit the gap<br>between operations, but I have no idea how that would be useful.</p><h2 id="Rate-vs-concurrency-limiting"><a href="#Rate-vs-concurrency-limiting" class="headerlink" title="Rate vs concurrency limiting"></a>Rate vs concurrency limiting</h2><p>While both rate and concurrency limits are trying to prevent a client from<br>overloading the server by making too many calls too fast, they do not replace<br>one another, and are implemented differently.</p><p>Suppose an API is rate-limited to 1 request per second. Even 1-concurrent requests<br>break the rate limit if they complete in under 1s. On the other hand, if the<br>requests take 3 seconds to complete, we can only have 3 of them running at the same time:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> ...</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>We could derive a bunch of formulae to connect the concurrency, rate and<br>running time of operations, but that’s completely beside the point. The thing to<br>remember here is that without strict guarantees on operation duration you can<br>not replace concurrency limit with rate limit or vice versa.</p><h2 id="Rate-limiting-individual-operations"><a href="#Rate-limiting-individual-operations" class="headerlink" title="Rate limiting individual operations"></a>Rate limiting individual operations</h2><p>The simplest form of rate limiting is “1 operation per N seconds”. This one is<br>straightforward, but first we need a building block — the promise counterpart<br>of <code>setTimeout</code>:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> resolveAfter = <span class="function"><span class="params">ms</span> =&gt;</span> <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function"><span class="params">ok</span> =&gt;</span> setTimeout(ok, ms));</span><br></pre></td></tr></table></figure><p><code>resolveAfter</code> is self-explanatory: it returns a promise that resolves after<br>the specified time has elapsed. Now, for the actual rate limiter:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">rateLimit1</span>(<span class="params">fn, msPerOp</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">let</span> wait = <span class="built_in">Promise</span>.resolve();</span><br><span class="line">  <span class="keyword">return</span> <span class="function">(<span class="params">...a</span>) =&gt;</span> &#123;</span><br><span class="line">    <span class="comment">// We use the queue tail in wait to start both the</span></span><br><span class="line">    <span class="comment">// next operation and the next delay</span></span><br><span class="line">    <span class="keyword">const</span> res = wait.then(<span class="function"><span class="params">()</span> =&gt;</span> fn(...a));</span><br><span class="line">    wait = wait.then(<span class="function"><span class="params">()</span> =&gt;</span> resolveAfter(msPerOp));</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Now we can, as usual, wrap the promise and call with no worries, the operations<br>are magically delayed:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> slowFetch = rateLimit1(fetch, <span class="number">1000</span>);</span><br><span class="line"><span class="built_in">Promise</span>.all(urls.map(<span class="function"><span class="params">u</span> =&gt;</span> slowFetch(u)))</span><br><span class="line">  .then(<span class="function"><span class="params">raw</span> =&gt;</span> <span class="built_in">Promise</span>.all(raw.map(<span class="function"><span class="params">p</span> =&gt;</span> p.json())))</span><br><span class="line">  .then(<span class="function"><span class="params">pages</span> =&gt;</span> <span class="built_in">console</span>.log(pages));</span><br></pre></td></tr></table></figure><p>The 1-rate-limiter can also be elegantly implemented on top of serializer<br>with the pitfall of unnecessarily delaying the first operation:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">rateLimit1</span>(<span class="params">fn, msPerOp</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">const</span> wait = serializePromises(<span class="function"><span class="params">()</span> =&gt;</span> resolveAfter(msPerOp));</span><br><span class="line">  <span class="keyword">return</span> <span class="function">(<span class="params">...a</span>) =&gt;</span> wait().then(<span class="function"><span class="params">()</span> =&gt;</span> fn(...a));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Rate-limiting-multiple-operations"><a href="#Rate-limiting-multiple-operations" class="headerlink" title="Rate limiting multiple operations"></a>Rate limiting multiple operations</h2><p>Many APIs feature soft rate limits instead: they allow <code>M request per N seconds</code>.<br>That is not equivalent to <code>1 request per N/M seconds</code>! Converting the multiple<br>rate limit into individual one does fulfil the rate limit, but is overly harsh<br>and non-optimal. Let’s see this through examples.</p><h3 id="Difference-from-individual-rate-limit-by-example"><a href="#Difference-from-individual-rate-limit-by-example" class="headerlink" title="Difference from individual rate limit, by example"></a>Difference from individual rate limit, by example</h3><p>Suppose you’re flying a plane, and the airline allows 10 kg of luggage per<br>passenger. If you’re travelling with a girl, and have one 16-kg bag with both<br>your things. At the check-in desk you’re asked to take out half the stuff in<br>your bag to make two 8-kg items. While formally correct, it feels idiotic —<br>you still add the exact same weight to the plane! But now, why would you enforce<br>such a stupid restriction on your own operations if you can do better?</p><p>Closer to the topic, let’s try 2-req-per-2-sec rate limit for operations<br>lasting 2 seconds. If you immediately fire 2 requests, you’re done in 2 seconds:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">----| 2 seconds, all done!</span><br><span class="line">----|</span><br></pre></td></tr></table></figure><p>Converting this into 1-req-per-1-sec delays the second request by 1s, and<br>now the same 2 requests take 3 seconds! You just lost a second for no reason.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">----  | 3 seconds</span><br><span class="line">  ----|</span><br></pre></td></tr></table></figure><h3 id="Understanding"><a href="#Understanding" class="headerlink" title="Understanding"></a>Understanding</h3><p>To understand what we should do, let’s have a closer look at the 1-rate-limit.<br>We essentially make a queue of promises that never resolve closer than <code>delay</code><br>apart. We use the resolutions to start the next operations, and don’t care<br>about its termination at all:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*--*--     *--*--</span><br></pre></td></tr></table></figure><p>This view extends to N-rate-limit: create N independent queues and put these<br>into a circular queue (yes, a queue of queues makes a good <em>in Soviet Russia</em><br>joke):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">*--*-- *--*--</span><br><span class="line"> *-- *--*--  *--</span><br><span class="line"> *--  *-- *--    *--</span><br></pre></td></tr></table></figure><p>The individual queues are unchanged, and never fire more than 1 operation per N<br>seconds. Thus, M queues can fire at most M operations during the window.</p><h3 id="Implementing"><a href="#Implementing" class="headerlink" title="Implementing"></a>Implementing</h3><p>With this plan in mind, we can generalize the implementation:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">rateLimit</span>(<span class="params">fn, windowMs, reqInWindow = <span class="number">1</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">// A battery of 1-rate-limiters</span></span><br><span class="line">  <span class="keyword">const</span> queue = _.range(reqInWindow).map(<span class="function"><span class="params">()</span> =&gt;</span> rateLimit1(fn, windowMs));</span><br><span class="line">  <span class="comment">// Circular queue cursor</span></span><br><span class="line">  <span class="keyword">let</span> i = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="function">(<span class="params">...a</span>) =&gt;</span> &#123;</span><br><span class="line">    <span class="comment">// to enqueue, we move the cursor...</span></span><br><span class="line">    i = (i + <span class="number">1</span>) % reqInWindow;</span><br><span class="line">    <span class="comment">// and return the rate-limited operation.</span></span><br><span class="line">    <span class="keyword">return</span> queue[i](...a);</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Preventing-queue-overflow"><a href="#Preventing-queue-overflow" class="headerlink" title="Preventing queue overflow"></a>Preventing queue overflow</h2><p>Just as before, we run into problems if the operations are consistently inserted<br>into the queue faster than the rate limit. The solution is the same: once the<br>queue exceeds the specified number of items, we immediately reject the incoming<br>operations.</p><h2 id="Combining-with-concurrency-limiting"><a href="#Combining-with-concurrency-limiting" class="headerlink" title="Combining with concurrency limiting"></a>Combining with concurrency limiting</h2><p>Now that we know how to limit both the rate and the number of simultaneously<br>running operations, and since neither is a substitute for another, we want a<br>way to combine the two limits. But can we build the joint rate/concurrency<br>limiter by composing the primitive limiters? Turns out we can, but should carefully<br>choose the order.</p><p><code>rateLimit(concurrencyLimit(fetch, N), ms)</code>, limits the rate at which the<br>operations enter the concurrency-limit queue. Serialized (1-concurrent) promises<br>rate-limited to 1 second break this combination. Suppose the first operation runs for<br>2 seconds, and during that time we throw 2 fast operations, O_2 and O_3 (say,<br>10 ms each) into the serializer. Instead of waiting for 1 second, the O_3 starts<br>right after O_2 completes, or 10ms after it starts, breaking the rate limit.</p><p><code>concurrencyLimit(rateLimit(fetch, ms), N)</code> limits the number of operations in<br>the rate-limit queue. Since the rate limiter only sees N operations at a time,<br>it has no chance to fire more than N, which is exactly what we want.<br>Hence, <strong>Chaining Rule 1: limit concurrency before rate.</strong></p><h2 id="Use-cases"><a href="#Use-cases" class="headerlink" title="Use cases"></a>Use cases</h2><p>The classic and most appropriate rate-limiting use case is for API requests.<br>But now that you know the pattern, you will see it in your own tasks and,<br>hopefully, use it ;-)</p><p>Promise-based rate limiting is a great way to quickly hack together a safe API<br>wrapper without depending on the underlying HTTP / TCP / WebSocket client.</p><p>Frankly, other use cases I can come up with off the top of my head (render<br>throttling and preventing too many e-mail notifications) are better served by<br>batching. Maybe, you’ll have better luck.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>We’ve learnt to rate-limit promise-based APIs, both for the simple<br>“1-action-per-N-seconds” and the more general M-actions case. Together with the<br>previously discussed concurrency limiter, these patterns allow us to build robust<br>service gateways with node.js, safely call external APIs and do all the other<br>things you come up with.</p><p>Planning note: I’ve decided to throw away the excessively tricky part on load<br>balancing and go with super fun and useful posts on <em>batching</em> and <em>handling failure</em>.<br>I have RSS now, so be sure to stay tuned!</p><p><strong>Advanced Promise Coordination Series</strong></p><ul><li><a href="/2018/06/20/serialize-promises/">Serialization and Concurrency Limiting</a></li><li><a href="/2018/07/07/rate-limit-promises/">Rate Limiting</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In the &lt;a href=&quot;/2018/06/20/serialize-promises/&quot;&gt;previous post&lt;/a&gt; we learnt to serialize&lt;br&gt;and concurrecy-limit promise-based operation
      
    
    </summary>
    
    
      <category term="javascript" scheme="https://thoughtspile.github.io/tags/javascript/"/>
    
      <category term="promises" scheme="https://thoughtspile.github.io/tags/promises/"/>
    
      <category term="programming" scheme="https://thoughtspile.github.io/tags/programming/"/>
    
      <category term="high availability" scheme="https://thoughtspile.github.io/tags/high-availability/"/>
    
  </entry>
  
  <entry>
    <title>Advanced Promises Coordination: Serialization and Concurrency Limiting</title>
    <link href="https://thoughtspile.github.io/2018/06/20/serialize-promises/"/>
    <id>https://thoughtspile.github.io/2018/06/20/serialize-promises/</id>
    <published>2018-06-20T09:42:08.000Z</published>
    <updated>2018-07-07T07:52:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>I’m sure you can chain promises with <code>doBefore().then(() =&gt; doAfter())</code> and even<br>run multiple promises in parallel using <code>Promise.any</code>. However, chaining an<br>unknown count of homogenous promises is trickier. Let me teach you to serialze<br>promises like a pro!</p><p>Suppose we want a list of all the cafes in a mid-sized european country.However,<br>the API only lets you query the cafes by city. No problem — we have a list of<br>all the cities, and will send a request for each one, then assemble the results.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> cities = [</span><br><span class="line">  <span class="string">"Abertamy"</span>,</span><br><span class="line">  <span class="string">"Adamov (Blansko District)"</span>,</span><br><span class="line">  <span class="string">"Aš"</span>,</span><br><span class="line">  <span class="string">"Bakov nad Jizerou"</span>,</span><br><span class="line">  <span class="string">"Bavorov"</span>,</span><br><span class="line">  <span class="string">"Bechyně"</span>,</span><br><span class="line">  <span class="string">"Bečov nad Teplou"</span>,</span><br><span class="line">  <span class="string">"Bělá nad Radbuzou"</span>,</span><br><span class="line">  <span class="string">"Bělá pod Bezdězem"</span>,</span><br><span class="line">  <span class="comment">// ... and 200 more</span></span><br><span class="line">];</span><br><span class="line"><span class="keyword">const</span> loadCafes = <span class="function"><span class="params">city</span> =&gt;</span> fetch(<span class="string">`api.fivecircle.com/city/<span class="subst">$&#123;city&#125;</span>`</span>);</span><br></pre></td></tr></table></figure><h2 id="How-Not-to-Chain-Promises"><a href="#How-Not-to-Chain-Promises" class="headerlink" title="How Not to Chain Promises"></a>How Not to Chain Promises</h2><p>The first naive attempts are no good:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// All gone in a glimpse of eye:</span></span><br><span class="line"><span class="built_in">Promise</span>.all(areas.map(loadCafes)).then(<span class="function"><span class="params">cafes</span> =&gt;</span> db.save(_.flatten(cafes)));</span><br><span class="line"><span class="comment">// Still not good</span></span><br><span class="line">areas.forEach(<span class="function"><span class="params">area</span> =&gt;</span> &#123;</span><br><span class="line">  loadCafes(area).then(storeData);</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// More of the same</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">let</span> area <span class="keyword">in</span> areas) &#123;</span><br><span class="line">  loadCafes(area).then(storeData);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Since promises start executing once created, each of these options fires all<br>the requests at once. With sane rate limiting restrictions, it will fail.<br>A less elaborate server could even crash.</p><p>We could, of course, use <code>await</code>:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> cafes = [];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">let</span> area <span class="keyword">of</span> areas) &#123;</span><br><span class="line">  cafes = cafes.concat(<span class="keyword">await</span> loadCafes(area));</span><br><span class="line">&#125;</span><br><span class="line">storeData(cafes);</span><br></pre></td></tr></table></figure><p>But I’m not a fan of this syntax — the code is now arguably C-like. I also<br>find error handling in promises cleaner. And now we have more preprocessing to do<br>for the code to work, which is nothing to be proud of. So let’s go on and write this<br>in pure promises instead.</p><h2 id="Explicit-Serialization"><a href="#Explicit-Serialization" class="headerlink" title="Explicit Serialization"></a>Explicit Serialization</h2><p>The best-known trick from this bunch is explicitly chaining an array of promises with<br><code>&lt;Array&gt;.reduce</code>. It works best for fire-and-forget promises, like redux actions:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> actions.reduce(</span><br><span class="line">  (pre, action) =&gt; before.then(<span class="function"><span class="params">()</span> =&gt;</span> action()),</span><br><span class="line">  <span class="built_in">Promise</span>.resolve());</span><br></pre></td></tr></table></figure><p>However, assembling return values is a bit awkward:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">areas.reduce(<span class="function">(<span class="params">before, area</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">return</span> before.then(<span class="function"><span class="params">acc</span> =&gt;</span> loadCafes(area).then(<span class="function"><span class="params">cafes</span> =&gt;</span> acc.concat(cafes)));</span><br><span class="line">&#125;, <span class="built_in">Promise</span>.resolve([])).then(<span class="function"><span class="params">cafes</span> =&gt;</span> db.save(cafes));</span><br></pre></td></tr></table></figure><p>Overall, this is good enough when you have an array of data you want to run the<br>actions on beforehand. But what if you don’t?</p><h2 id="Implicit-Serialization"><a href="#Implicit-Serialization" class="headerlink" title="Implicit Serialization"></a>Implicit Serialization</h2><p>We can actually write a wrapper for arbitrary promise-returning<br>functions that makes any call wait for the previous ones to finish. This wrapper<br>is completely transparent, leaving the function’s interface intact — good for<br>composability. Here it is:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">serializePromises</span>(<span class="params">immediate</span>) </span>&#123;</span><br><span class="line">  <span class="comment">// This works as our promise queue</span></span><br><span class="line">  <span class="keyword">let</span> last = <span class="built_in">Promise</span>.resolve();</span><br><span class="line">  <span class="keyword">return</span> <span class="function"><span class="keyword">function</span> (<span class="params">...a</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// Catch is necessary here — otherwise a rejection in a promise will</span></span><br><span class="line">    <span class="comment">// break the serializer forever</span></span><br><span class="line">    last = last.catch(<span class="function"><span class="params">()</span> =&gt;</span> &#123;&#125;).then(<span class="function"><span class="params">()</span> =&gt;</span> immediate(...a));</span><br><span class="line">    <span class="keyword">return</span> last;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Now we can just wrap our function and never have to worry about flooding the API again:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> loadCafesSafe = serializePromises(loadCafes);</span><br><span class="line"><span class="built_in">Promise</span>.all(areas.map(<span class="function"><span class="params">a</span> =&gt;</span> loadCafesSafe(a)));</span><br></pre></td></tr></table></figure><p>It’s so easy it doesn’t warrant a library — just five lines of code. And we can<br>take this idea further with…</p><h2 id="Concurrency-Limiting"><a href="#Concurrency-Limiting" class="headerlink" title="Concurrency Limiting"></a>Concurrency Limiting</h2><p>Serialization effectively forces our promises to run in one thread. To make them<br>go faster, we can generalize the serializer to allow not one, but at most N<br>promises to run simultaneously:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">limitConcurrency</span>(<span class="params">immediate, maxConcurrent</span>) </span>&#123;</span><br><span class="line">  <span class="comment">// Each element holds its index, or a promise resolving with the index</span></span><br><span class="line">  <span class="keyword">const</span> workers = _.range(maxConcurrent);</span><br><span class="line">  <span class="comment">// Without this serialization, Promise.race would resolve with the same</span></span><br><span class="line">  <span class="comment">// worker whenever a concurrency-limited function was synchronously called</span></span><br><span class="line">  <span class="comment">// multiple times.</span></span><br><span class="line">  <span class="keyword">const</span> findWorker = serializePromises(<span class="function"><span class="params">()</span> =&gt;</span> <span class="built_in">Promise</span>.race(workers));</span><br><span class="line">  <span class="keyword">return</span> <span class="function"><span class="keyword">function</span> (<span class="params">...a</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// race resolves with the first free worker</span></span><br><span class="line">    <span class="keyword">return</span> findWorker().then(<span class="function"><span class="params">i</span> =&gt;</span> &#123;</span><br><span class="line">      <span class="comment">// and here we start the action and update the worker correspondingly:</span></span><br><span class="line">      <span class="keyword">const</span> promise = immediate(...a);</span><br><span class="line">      workers[i] = promise.then(<span class="function"><span class="params">()</span> =&gt;</span> i, () =&gt; i);</span><br><span class="line">      <span class="keyword">return</span> promise;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The idea is the same, but we replaced the single <code>last</code> promise with an array of<br>N workers and added some bookkeeping. This code packs promises into threads as<br>tightly as possible, with no idle time.</p><p>Also note that <code>serializePromises</code> is the same as <code>a =&gt; limitConcurrency(a, 1)</code>.</p><p>If you want to impose joint limiting on several arbitrary functions, you can tweak the<br>code — I leave this to you as an exercise ;-)</p><h2 id="Propagating-Rate-Errors"><a href="#Propagating-Rate-Errors" class="headerlink" title="Propagating Rate Errors"></a>Propagating Rate Errors</h2><p>Now that our code manages a promise queue, we can see a potential problem in it.<br>The system can smooth activity spikes without propagating these upstream.<br>However, if the request rate is higher than what the upstream can handle for an<br>extended period of time, our queue can overfill and blow up the memory limit.</p><p>The problem still existed before we added the limiter, but would occurred<br>upstream instead. No wrapper can magically improve service throughput.</p><p>To handle these errors without crashing our process, we can put a hard limit on<br>queue size. Here’s how it can be done for the generic <code>limitConcurrency</code>:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">limitConcurrency</span>(<span class="params">immediate, maxConcurrent, maxQueue</span>) </span>&#123;</span><br><span class="line">  <span class="comment">// this is our queue counter</span></span><br><span class="line">  <span class="keyword">let</span> queued = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">const</span> workers = _.range(maxConcurrent);</span><br><span class="line">  <span class="keyword">const</span> findWorker = serializePromises(<span class="function"><span class="params">()</span> =&gt;</span> <span class="built_in">Promise</span>.race(workers));</span><br><span class="line">  <span class="keyword">return</span> <span class="function"><span class="keyword">function</span> (<span class="params">...a</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (queued &gt;= maxQueue) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">Promise</span>.reject(<span class="keyword">new</span> <span class="built_in">Error</span>(<span class="string">'Max queue size reached'</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    queued += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> findWorker().then(<span class="function"><span class="params">i</span> =&gt;</span> &#123;</span><br><span class="line">      queued -= <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">const</span> promise = immediate(...a);</span><br><span class="line">      workers[i] = promise.then(<span class="function"><span class="params">()</span> =&gt;</span> i, () =&gt; i));</span><br><span class="line">      <span class="keyword">return</span> promise;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Now, instead of uncontrollably enqueueing, the coordinator rejects when there’s<br>already too much work ahead. The consumers can handle these errors and retry later.</p><h2 id="Use-Cases"><a href="#Use-Cases" class="headerlink" title="Use Cases"></a>Use Cases</h2><p>So far we’ve been discussing an example with API requests, and you might argue<br>that concurrency limiting functionality should be provided bt the HTTP client<br>library. That’s true, but the power of our promise-based strategy is its generality.<br>Here are some unorthodox use cases:</p><h3 id="“Sloppy-Transactions”-with-Serialization"><a href="#“Sloppy-Transactions”-with-Serialization" class="headerlink" title="“Sloppy Transactions” with Serialization"></a>“Sloppy Transactions” with Serialization</h3><p>Suppose an action involves reading an external data source, computing on the<br>response and issuing an update. If the source changes between the read and the<br>update, you’ve corrupted your data beyond repair. You can instead wrap the action<br>with our “promise serializer”. Of course, this assumes that the relevant data is only<br>accessed by your wrapper, and only by a single process. You can even build a<br>simple file-based database.</p><h3 id="Prevent-Notification-Flood-with-Concurrency-Limiting"><a href="#Prevent-Notification-Flood-with-Concurrency-Limiting" class="headerlink" title="Prevent Notification Flood with Concurrency Limiting"></a>Prevent Notification Flood with Concurrency Limiting</h3><p>A front-end idea. You probably have a notification area somewhere on<br>the screen. However, if a large batch of notifications just arrived, the users are<br>likely to miss some of those. But now you can treat the currently visible<br>notifications as the running threads and apply <code>limitConcurrecny</code>!</p><p>A similar use case for modal windows uses serialized promises — you can’t<br>show multiple modals at the same time, but now you can enqueue the next one<br>instead.</p><h3 id="Web-Worker-Thread-Pool"><a href="#Web-Worker-Thread-Pool" class="headerlink" title="Web Worker Thread Pool"></a>Web Worker Thread Pool</h3><p>Finally, time for some cutting-edge tech. If your web app heavily uses web<br>workers for background processing, you can wrap web worker access with a<br>promise-based API, then use our wrapper to limit the number of simultaneously<br>active workers. With several kinds of specialized workers, you might choose to<br>use a multi-factory flavour of our <code>limitConcurrecny</code> instead. I’ll delve<br>deeper into this this case with an upcoming article on load balancing.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>We’ve learnt how to force promises to run consecutively and even to limit the<br>number of pending promises to a specified number. This technique can be used<br>for safer back-end access, and its generality allows to use it for any<br>promise-based API.</p><p>I’m not too good at writing: the topics kept expanding in my head, and I have<br>had a hard time finishing this article. I have two other interesting<br>promise coordination patterns to handle in future articles of this series:</p><ul><li>Rate Limiting</li><li>Load Balancing</li></ul><p>Wish me luck writing these! If you have some tips or want to argue, drop me an<br>e-mail.</p><p><strong>Advanced Promise Coordination Series</strong></p><ul><li><a href="/2018/06/20/serialize-promises/">Serialization and Concurrency Limiting</a></li><li><a href="/2018/07/07/rate-limit-promises/">Rate Limiting</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;I’m sure you can chain promises with &lt;code&gt;doBefore().then(() =&amp;gt; doAfter())&lt;/code&gt; and even&lt;br&gt;run multiple promises in parallel using
      
    
    </summary>
    
    
      <category term="javascript" scheme="https://thoughtspile.github.io/tags/javascript/"/>
    
      <category term="promises" scheme="https://thoughtspile.github.io/tags/promises/"/>
    
      <category term="programming" scheme="https://thoughtspile.github.io/tags/programming/"/>
    
      <category term="concurrency" scheme="https://thoughtspile.github.io/tags/concurrency/"/>
    
  </entry>
  
</feed>
